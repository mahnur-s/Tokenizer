{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J0eubdIraNJW"
      },
      "outputs": [],
      "source": [
        "# Use python 3.7 or later (IMPORTANT)\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import regex as re\n",
        "import math\n",
        "# set seed\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ClHq17caNJX"
      },
      "source": [
        "We use Jane Austen's \"Emma\" novel as the running example for the rest of our homework\n",
        "\n",
        "> **Add blockquote**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xgg0zCKvaNJY"
      },
      "outputs": [],
      "source": [
        "## Load data\n",
        "with open(\"./Emma.txt\", \"r\", encoding='UTF8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw8qH5y4aNJY",
        "outputId": "c057449e-2088-463f-fff6-8999062aa578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total: 899644 characters\n",
            "number of unique characthers 92\n"
          ]
        }
      ],
      "source": [
        "# see dataset properties:\n",
        "print(f\"Total: {len(list(text))} characters\")\n",
        "print(f\"number of unique characthers {len(set(text))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPWLe-68aNJa"
      },
      "source": [
        "However, the Emma dataset is quite big. So, we will use this small dataset from [NYTimes](https://www.nytimes.com/2024/09/04/opinion/yuval-harari-ai-democracy.html) for debugging purpose. Use this only for debugging and not for reporting final results. Do your experiments with Emma dataset and the data kept in the multilingual-data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NaQF0OCzaNJa"
      },
      "outputs": [],
      "source": [
        "text_ = f\"\"\"Democracy is a conversation. Its function and survival depend on the available information technology. For most of history, no technology existed for holding large-scale conversations among millions of people. In the premodern world, democracies existed only in small city-states like Rome and Athens, or in even smaller tribes. Once a polity grew large, the democratic conversation collapsed, and authoritarianism remained the only alternative.\n",
        "\n",
        "Large-scale democracies became feasible only after the rise of modern information technologies like the newspaper, the telegraph and the radio. The fact that modern democracy has been built on top of modern information technologies means that any major change in the underlying technology is likely to result in a political upheaval.\n",
        "\n",
        "This partly explains the current worldwide crisis of democracy. In the United States, Democrats and Republicans can hardly agree on even the most basic facts, such as who won the 2020 presidential election. A similar breakdown is happening in numerous other democracies around the world, from Brazil to Israel and from France to the Philippines.\n",
        "\n",
        "In the early days of the internet and social media, tech enthusiasts promised they would spread truth, topple tyrants and ensure the universal triumph of liberty. So far, they seem to have had the opposite effect. We now have the most sophisticated information technology in history, but we are losing the ability to talk with one another, and even more so the ability to listen.\n",
        "\n",
        "As technology has made it easier than ever to spread information, attention became a scarce resource, and the ensuing battle for attention resulted in a deluge of toxic information. But the battle lines are now shifting from attention to intimacy. The new generative artificial intelligence is capable of not only producing texts, images and videos, but also conversing with us directly, pretending to be human.\n",
        "\n",
        "Over the past two decades, algorithms fought algorithms to grab attention by manipulating conversations and content. In particular, algorithms tasked with maximizing user engagement discovered by experimenting on millions of human guinea pigs that if you press the greed, hate or fear button in the brain, you grab the attention of that human and keep that person glued to the screen. The algorithms began to deliberately promote such content. But the algorithms had only limited capacity to produce this content by themselves or to directly hold an intimate conversation. This is now changing, with the introduction of generative A.I.s like OpenAI’s GPT-4.\n",
        "\"\"\"\n",
        "\n",
        "# The author(s) of this homework do not necessarily agree/endorse the views expressed in the text above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K0tE7YZaNJc"
      },
      "source": [
        "\n",
        "Let's start with implementing the simplest form of the tokenizer: character level tokenizer. In fact many early NLP research were conducted using character level tokenization.\n",
        "\n",
        "Notice the CharTokenizer class below. For the rest of the homework, your implementation will follow a similar structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zME0-GB4aNJc"
      },
      "outputs": [],
      "source": [
        "class CharTokenizer:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        All of your tokenizer implementation should have tokenizer_map attribute\n",
        "        tokenizer_map is a dictionary that maps individual numbers to corresponding tokens\n",
        "        map_of_char maps individual characters to corresponding token indices. In this particular case, it is the reverse of tokenizer_map; however, it is not always the case. map_of_char is an optional attribute for your tokenizer implementation. You may not need it for your implementation.\n",
        "        \"\"\"\n",
        "        self.tokenizer_map = None\n",
        "        self.map_of_char = None\n",
        "\n",
        "\n",
        "    def train(self, text):\n",
        "\n",
        "        self.character_set = set(text)\n",
        "        self.tokenizer_map = {i: char for i, char in enumerate(self.character_set)}\n",
        "        self.map_of_char = {char: i for i, char in self.tokenizer_map.items()}\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        return [self.map_of_char[char] for char in text]\n",
        "\n",
        "    def decode_text(self, encoded_text):\n",
        "        return \"\".join([list(self.map_of_char.keys())[i] for i in encoded_text])\n",
        "\n",
        "chat_tokenizer = CharTokenizer()\n",
        "chat_tokenizer.train(text_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmob5Nv_aNJd"
      },
      "source": [
        "Now see the usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjLX5CH1aNJd",
        "outputId": "149de89e-37c7-4790-b913-c83ff46c4238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded text:\n",
            "\n",
            "Democracy is a conversation. Its function and survival depend on the available information technology. For most of history, no technology existed for holding large-scale conversations among millions of people. In the premodern world, democracies existed only in small city-states like Rome and Athens, or in even smaller tribes. Once a polity grew large, the democratic conversation collapsed, and authoritarianism remained the only alternative.\n",
            "\n",
            "Large-scale democracies became feasible only after the rise of modern information technologies like the newspaper, the telegraph and the radio. The fact that modern democracy has been built on top of modern information technologies means that any major change in the underlying technology is likely to result in a political upheaval.\n",
            "\n",
            "This partly explains the current worldwide crisis of democracy. In the United States, Democrats and Republicans can hardly agree on even the most basic facts, such as who won the 2020 presidential election. A similar breakdown is happening in numerous other democracies around the world, from Brazil to Israel and from France to the Philippines.\n",
            "\n",
            "In the early days of the internet and social media, tech enthusiasts promised they would spread truth, topple tyrants and ensure the universal triumph of liberty. So far, they seem to have had the opposite effect. We now have the most sophisticated information technology in history, but we are losing the ability to talk with one another, and even more so the ability to listen.\n",
            "\n",
            "As technology has made it easier than ever to spread information, attention became a scarce resource, and the ensuing battle for attention resulted in a deluge of toxic information. But the battle lines are now shifting from attention to intimacy. The new generative artificial intelligence is capable of not only producing texts, images and videos, but also conversing with us directly, pretending to be human.\n",
            "\n",
            "Over the past two decades, algorithms fought algorithms to grab attention by manipulating conversations and content. In particular, algorithms tasked with maximizing user engagement discovered by experimenting on millions of human guinea pigs that if you press the greed, hate or fear button in the brain, you grab the attention of that human and keep that person glued to the screen. The algorithms began to deliberately promote such content. But the algorithms had only limited capacity to produce this content by themselves or to directly hold an intimate conversation. This is now changing, with the introduction of generative A.I.s like OpenAI’s GPT-4.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# First, initialize the tokenizer\n",
        "char_tokenizer = CharTokenizer()\n",
        "\n",
        "# Then train the tokenizer on the Emma text\n",
        "char_tokenizer.train(text)\n",
        "\n",
        "# Now encode some texts using the trained tokenizer\n",
        "enc = char_tokenizer.encode_text(text_) # notice that we are using text_ here\n",
        "\n",
        "# Now decode the encoded text\n",
        "print(f\"Decoded text:\\n\\n{char_tokenizer.decode_text(enc)}\")\n",
        "\n",
        "# If the decoded text is the same as the original text, then the implementation is correct\n",
        "# use this sanity check to verify all of your implementations\n",
        "assert char_tokenizer.decode_text(char_tokenizer.encode_text(text_)) == text_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZURO-S9FaNJd"
      },
      "source": [
        "However, in reality, we do not use character level tokenization. What we use is subword level tokenization. This is why you will implement one of the most effective and popular subword level tokenization method called BPE algorithm.\n",
        "\n",
        "\n",
        "First, you will implement the vanilla version of it.\n",
        "\n",
        "## 1. Vanilla BPETokenizer:\n",
        "This version does *not* have a preprocessing step. This is also called SentencePiece tokenization (do not confuse with the library with the same name) and was introduced into [this paper](https://arxiv.org/pdf/1808.06226)\n",
        "Check out this pseudocode for a clearer understanding:\n",
        "\n",
        "![Vanilla BPETokenizer](./images/vanilla%20BPE.png)\n",
        "\n",
        "Note that when multiple bigrams have same frequency, ties can be broken by picking a random one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "72CsYjpfaNJe"
      },
      "outputs": [],
      "source": [
        "class BPETokenizer:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        READ THIS DESCRIPTION:\n",
        "\n",
        "        map_of_merged_tokens: is a dictionary that maps the merged bigram to a single token. According to the notations in the pseudocode above, it is a dictionary with keys (v_i, v_j) and values v_n. You need to keep this dictionary updated as you merge tokens in the training step. This dictionary is needed when decoding. When done right, it should look something like {(5, 68): 92, (68, 23): 93, (68, 35): 94, ...}. Here it means that token 5 and 68 were merged to form token 92, 68 and 23 were merged to form token 93, and so on. *The order in the dictionary refers to the order of merging.*\n",
        "        tokenizer_map: is a dictionary that maps token numbers to their corresponding tokens (subwords). Its length should be equal to the target_vocab_size. After a successful training, tokenizer_map might look something like {..., '995: 'uten',996: 'ion of ', 997: ' turn', ...} Notice that this behavior is opposite to get_vocab method in huggingface tokenizers, where the subwords are the keys and the values are the token numbers.\n",
        "        map_of_char: maps individual characters to corresponding token indices. Optional. Use it only if your implementation needs.\n",
        "        \"\"\"\n",
        "        self.map_of_merged_tokens = None\n",
        "        self.tokenizer_map = None\n",
        "        self.map_of_char = None\n",
        "\n",
        "    @staticmethod\n",
        "    def replace_pairs(lst:list, bigram:tuple, c: int):\n",
        "\n",
        "        \"\"\"\n",
        "        Helper function to replace bigram with a single token in the list.\n",
        "        Whether to use this particular function or not depends on *your* implementations.\n",
        "        It is possible that you may not need this function at all.\n",
        "        \"\"\"\n",
        "        i = 0\n",
        "        while i < len(lst) - 1:\n",
        "            if lst[i] == bigram[0] and lst[i + 1] == bigram[1]:\n",
        "                lst[i] = c\n",
        "                del lst[i + 1]\n",
        "            else:\n",
        "                i += 1\n",
        "        return lst\n",
        "\n",
        "    def train(self, text: str, target_vocab_size: int):\n",
        "      self.target_vocab_size = target_vocab_size\n",
        "      character_set = set(text)\n",
        "      vocabsize = len(character_set)\n",
        "      assert self.target_vocab_size > vocabsize, \"target vocab size must be greater than the number of unique characters in the text\"\n",
        "\n",
        "      self.map_of_char = {char: i for i, char in enumerate(character_set)}\n",
        "      map_of_merged_tokens = {}\n",
        "      tokenized = [self.map_of_char[char] for char in text]\n",
        "\n",
        "      current_token_id = max(self.map_of_char.values()) + 1\n",
        "      while vocabsize < target_vocab_size:\n",
        "          bigram_counts = Counter((tokenized[i], tokenized[i + 1]) for i in range(len(tokenized) - 1))\n",
        "\n",
        "          # Checking to see if bigram_counts is empty\n",
        "          if not bigram_counts:\n",
        "              break\n",
        "\n",
        "          most_common_frequency = bigram_counts.most_common(1)[0][1]  #finding most common frequency\n",
        "          most_frequent_bigrams = [bigram for bigram, count in bigram_counts.items() if count == most_common_frequency]\n",
        "\n",
        "          # Another check to ensure there is at least one frequent bigram\n",
        "          if not most_frequent_bigrams:\n",
        "              break\n",
        "\n",
        "          selected_bigram = random.choice(most_frequent_bigrams)\n",
        "          map_of_merged_tokens[selected_bigram] = current_token_id\n",
        "          tokenized = self.replace_pairs(tokenized, selected_bigram, current_token_id)\n",
        "\n",
        "          if self.tokenizer_map is None:\n",
        "              self.tokenizer_map = {}\n",
        "          self.tokenizer_map[current_token_id] = selected_bigram\n",
        "          current_token_id += 1\n",
        "          vocabsize += 1\n",
        "\n",
        "      self.map_of_merged_tokens = map_of_merged_tokens\n",
        "      self.tokenizer_map = self.tokenizer_map\n",
        "\n",
        "    def encode_text(self, text: str):\n",
        "\n",
        "        ### TO DO: Write the implementation of the encode_text function ###\n",
        "        \"\"\"Given a text, return the tokenized text as a list of integers\"\"\"\n",
        "        tokenized_text = [self.map_of_char[char] for char in text]\n",
        "        tokenized = []\n",
        "\n",
        "        i = 0\n",
        "        while i < len(tokenized_text):\n",
        "            if i < len(tokenized_text) - 1:\n",
        "                bigram = (tokenized_text[i], tokenized_text[i + 1])\n",
        "                if bigram in self.map_of_merged_tokens:\n",
        "                    tokenized.append(self.map_of_merged_tokens[bigram])\n",
        "                    i += 2\n",
        "                    continue\n",
        "            tokenized.append(tokenized_text[i])\n",
        "            i += 1\n",
        "\n",
        "        ### END ###\n",
        "        return tokenized\n",
        "\n",
        "    def decode_text(self, tokenized_text: list):\n",
        "        \"\"\"Given a tokenized text, return the original text as a string\"\"\"\n",
        "        ### TO DO: Write the implementation of the decode_text function ###\n",
        "        decoded = []\n",
        "        reverse_map = {v: k for k, v in self.map_of_char.items()}\n",
        "\n",
        "        for token in tokenized_text:\n",
        "            if token in self.tokenizer_map:\n",
        "                subword = self.tokenizer_map[token]\n",
        "                decoded.extend(reverse_map[char] for char in subword)\n",
        "            elif token in reverse_map:\n",
        "                decoded.append(reverse_map[token])\n",
        "\n",
        "        return \"\".join(decoded)\n",
        "        ### END ###\n",
        "        return decoded\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WMalGUgLaNJe"
      },
      "outputs": [],
      "source": [
        "tokenizer = BPETokenizer()\n",
        "\n",
        "tokenizer.train(text_, 100) # just for checking if the training is running or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iGSkb9F-aNJe"
      },
      "outputs": [],
      "source": [
        "encoded_text = tokenizer.encode_text(\"Life is beautiful\")\n",
        "tokenizer.decode_text(encoded_text)\n",
        "assert tokenizer.decode_text(tokenizer.encode_text(\"Life is beautiful\")) == \"Life is beautiful\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO4vCso1aNJf"
      },
      "source": [
        "### 2. BPE Tokenizer with preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOw32srYaNJf"
      },
      "source": [
        "However, in reality we apply a preprocessing step before the BPE tokenization pipeline.\n",
        "\n",
        "In the preprocessing step, we split the text with a complex regex pattern to separate the texts on the space and punctuation boundary.\n",
        "\n",
        "To see the effect of the regex, see this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mkElvUyAaNJf",
        "outputId": "16bcd3d6-a34f-46af-a3bd-50c7a24c15fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Democracy',\n",
              " ' is',\n",
              " ' a',\n",
              " ' conversation',\n",
              " '.',\n",
              " ' Its',\n",
              " ' function',\n",
              " ' and',\n",
              " ' survival',\n",
              " ' depend',\n",
              " ' on',\n",
              " ' the',\n",
              " ' available',\n",
              " ' information',\n",
              " ' technology',\n",
              " '.',\n",
              " ' For',\n",
              " ' most',\n",
              " ' of',\n",
              " ' history',\n",
              " ',',\n",
              " ' no',\n",
              " ' technology',\n",
              " ' existed',\n",
              " ' for',\n",
              " ' holding',\n",
              " ' large',\n",
              " '-scale',\n",
              " ' conversations',\n",
              " ' among',\n",
              " ' millions',\n",
              " ' of',\n",
              " ' people',\n",
              " '.',\n",
              " ' In',\n",
              " ' the',\n",
              " ' premodern',\n",
              " ' world',\n",
              " ',',\n",
              " ' democracies',\n",
              " ' existed',\n",
              " ' only',\n",
              " ' in',\n",
              " ' small',\n",
              " ' city',\n",
              " '-states',\n",
              " ' like',\n",
              " ' Rome',\n",
              " ' and',\n",
              " ' Athens',\n",
              " ',',\n",
              " ' or',\n",
              " ' in',\n",
              " ' even',\n",
              " ' smaller',\n",
              " ' tribes',\n",
              " '.',\n",
              " ' Once',\n",
              " ' a',\n",
              " ' polity',\n",
              " ' grew',\n",
              " ' large',\n",
              " ',',\n",
              " ' the',\n",
              " ' democratic',\n",
              " ' conversation',\n",
              " ' collapsed',\n",
              " ',',\n",
              " ' and',\n",
              " ' authoritarianism',\n",
              " ' remained',\n",
              " ' the',\n",
              " ' only',\n",
              " ' alternative',\n",
              " '.\\n\\n',\n",
              " 'Large',\n",
              " '-scale',\n",
              " ' democracies',\n",
              " ' became',\n",
              " ' feasible',\n",
              " ' only',\n",
              " ' after',\n",
              " ' the',\n",
              " ' rise',\n",
              " ' of',\n",
              " ' modern',\n",
              " ' information',\n",
              " ' technologies',\n",
              " ' like',\n",
              " ' the',\n",
              " ' newspaper',\n",
              " ',',\n",
              " ' the',\n",
              " ' telegraph',\n",
              " ' and',\n",
              " ' the',\n",
              " ' radio',\n",
              " '.',\n",
              " ' The',\n",
              " ' fact',\n",
              " ' that',\n",
              " ' modern',\n",
              " ' democracy',\n",
              " ' has',\n",
              " ' been',\n",
              " ' built',\n",
              " ' on',\n",
              " ' top',\n",
              " ' of',\n",
              " ' modern',\n",
              " ' information',\n",
              " ' technologies',\n",
              " ' means',\n",
              " ' that',\n",
              " ' any',\n",
              " ' major',\n",
              " ' change',\n",
              " ' in',\n",
              " ' the',\n",
              " ' underlying',\n",
              " ' technology',\n",
              " ' is',\n",
              " ' likely',\n",
              " ' to',\n",
              " ' result',\n",
              " ' in',\n",
              " ' a',\n",
              " ' political',\n",
              " ' upheaval',\n",
              " '.\\n\\n',\n",
              " 'This',\n",
              " ' partly',\n",
              " ' explains',\n",
              " ' the',\n",
              " ' current',\n",
              " ' worldwide',\n",
              " ' crisis',\n",
              " ' of',\n",
              " ' democracy',\n",
              " '.',\n",
              " ' In',\n",
              " ' the',\n",
              " ' United',\n",
              " ' States',\n",
              " ',',\n",
              " ' Democrats',\n",
              " ' and',\n",
              " ' Republicans',\n",
              " ' can',\n",
              " ' hardly',\n",
              " ' agree',\n",
              " ' on',\n",
              " ' even',\n",
              " ' the',\n",
              " ' most',\n",
              " ' basic',\n",
              " ' facts',\n",
              " ',',\n",
              " ' such',\n",
              " ' as',\n",
              " ' who',\n",
              " ' won',\n",
              " ' the',\n",
              " ' ',\n",
              " '202',\n",
              " '0',\n",
              " ' presidential',\n",
              " ' election',\n",
              " '.',\n",
              " ' A',\n",
              " ' similar',\n",
              " ' breakdown',\n",
              " ' is',\n",
              " ' happening',\n",
              " ' in',\n",
              " ' numerous',\n",
              " ' other',\n",
              " ' democracies',\n",
              " ' around',\n",
              " ' the',\n",
              " ' world',\n",
              " ',',\n",
              " ' from',\n",
              " ' Brazil',\n",
              " ' to',\n",
              " ' Israel',\n",
              " ' and',\n",
              " ' from',\n",
              " ' France',\n",
              " ' to',\n",
              " ' the',\n",
              " ' Philippines',\n",
              " '.\\n\\n',\n",
              " 'In',\n",
              " ' the',\n",
              " ' early',\n",
              " ' days',\n",
              " ' of',\n",
              " ' the',\n",
              " ' internet',\n",
              " ' and',\n",
              " ' social',\n",
              " ' media',\n",
              " ',',\n",
              " ' tech',\n",
              " ' enthusiasts',\n",
              " ' promised',\n",
              " ' they',\n",
              " ' would',\n",
              " ' spread',\n",
              " ' truth',\n",
              " ',',\n",
              " ' topple',\n",
              " ' tyrants',\n",
              " ' and',\n",
              " ' ensure',\n",
              " ' the',\n",
              " ' universal',\n",
              " ' triumph',\n",
              " ' of',\n",
              " ' liberty',\n",
              " '.',\n",
              " ' So',\n",
              " ' far',\n",
              " ',',\n",
              " ' they',\n",
              " ' seem',\n",
              " ' to',\n",
              " ' have',\n",
              " ' had',\n",
              " ' the',\n",
              " ' opposite',\n",
              " ' effect',\n",
              " '.',\n",
              " ' We',\n",
              " ' now',\n",
              " ' have',\n",
              " ' the',\n",
              " ' most',\n",
              " ' sophisticated',\n",
              " ' information',\n",
              " ' technology',\n",
              " ' in',\n",
              " ' history',\n",
              " ',',\n",
              " ' but',\n",
              " ' we',\n",
              " ' are',\n",
              " ' losing',\n",
              " ' the',\n",
              " ' ability',\n",
              " ' to',\n",
              " ' talk',\n",
              " ' with',\n",
              " ' one',\n",
              " ' another',\n",
              " ',',\n",
              " ' and',\n",
              " ' even',\n",
              " ' more',\n",
              " ' so',\n",
              " ' the',\n",
              " ' ability',\n",
              " ' to',\n",
              " ' listen',\n",
              " '.\\n\\n',\n",
              " 'As',\n",
              " ' technology',\n",
              " ' has',\n",
              " ' made',\n",
              " ' it',\n",
              " ' easier',\n",
              " ' than',\n",
              " ' ever',\n",
              " ' to',\n",
              " ' spread',\n",
              " ' information',\n",
              " ',',\n",
              " ' attention',\n",
              " ' became',\n",
              " ' a',\n",
              " ' scarce',\n",
              " ' resource',\n",
              " ',',\n",
              " ' and',\n",
              " ' the',\n",
              " ' ensuing',\n",
              " ' battle',\n",
              " ' for',\n",
              " ' attention',\n",
              " ' resulted',\n",
              " ' in',\n",
              " ' a',\n",
              " ' deluge',\n",
              " ' of',\n",
              " ' toxic',\n",
              " ' information',\n",
              " '.',\n",
              " ' But',\n",
              " ' the',\n",
              " ' battle',\n",
              " ' lines',\n",
              " ' are',\n",
              " ' now',\n",
              " ' shifting',\n",
              " ' from',\n",
              " ' attention',\n",
              " ' to',\n",
              " ' intimacy',\n",
              " '.',\n",
              " ' The',\n",
              " ' new',\n",
              " ' generative',\n",
              " ' artificial',\n",
              " ' intelligence',\n",
              " ' is',\n",
              " ' capable',\n",
              " ' of',\n",
              " ' not',\n",
              " ' only',\n",
              " ' producing',\n",
              " ' texts',\n",
              " ',',\n",
              " ' images',\n",
              " ' and',\n",
              " ' videos',\n",
              " ',',\n",
              " ' but',\n",
              " ' also',\n",
              " ' conversing',\n",
              " ' with',\n",
              " ' us',\n",
              " ' directly',\n",
              " ',',\n",
              " ' pretending',\n",
              " ' to',\n",
              " ' be',\n",
              " ' human',\n",
              " '.\\n\\n',\n",
              " 'Over',\n",
              " ' the',\n",
              " ' past',\n",
              " ' two',\n",
              " ' decades',\n",
              " ',',\n",
              " ' algorithms',\n",
              " ' fought',\n",
              " ' algorithms',\n",
              " ' to',\n",
              " ' grab',\n",
              " ' attention',\n",
              " ' by',\n",
              " ' manipulating',\n",
              " ' conversations',\n",
              " ' and',\n",
              " ' content',\n",
              " '.',\n",
              " ' In',\n",
              " ' particular',\n",
              " ',',\n",
              " ' algorithms',\n",
              " ' tasked',\n",
              " ' with',\n",
              " ' maximizing',\n",
              " ' user',\n",
              " ' engagement',\n",
              " ' discovered',\n",
              " ' by',\n",
              " ' experimenting',\n",
              " ' on',\n",
              " ' millions',\n",
              " ' of',\n",
              " ' human',\n",
              " ' guinea',\n",
              " ' pigs',\n",
              " ' that',\n",
              " ' if',\n",
              " ' you',\n",
              " ' press',\n",
              " ' the',\n",
              " ' greed',\n",
              " ',',\n",
              " ' hate',\n",
              " ' or',\n",
              " ' fear',\n",
              " ' button',\n",
              " ' in',\n",
              " ' the',\n",
              " ' brain',\n",
              " ',',\n",
              " ' you',\n",
              " ' grab',\n",
              " ' the',\n",
              " ' attention',\n",
              " ' of',\n",
              " ' that',\n",
              " ' human',\n",
              " ' and',\n",
              " ' keep',\n",
              " ' that',\n",
              " ' person',\n",
              " ' glued',\n",
              " ' to',\n",
              " ' the',\n",
              " ' screen',\n",
              " '.',\n",
              " ' The',\n",
              " ' algorithms',\n",
              " ' began',\n",
              " ' to',\n",
              " ' deliberately',\n",
              " ' promote',\n",
              " ' such',\n",
              " ' content',\n",
              " '.',\n",
              " ' But',\n",
              " ' the',\n",
              " ' algorithms',\n",
              " ' had',\n",
              " ' only',\n",
              " ' limited',\n",
              " ' capacity',\n",
              " ' to',\n",
              " ' produce',\n",
              " ' this',\n",
              " ' content',\n",
              " ' by',\n",
              " ' themselves',\n",
              " ' or',\n",
              " ' to',\n",
              " ' directly',\n",
              " ' hold',\n",
              " ' an',\n",
              " ' intimate',\n",
              " ' conversation',\n",
              " '.',\n",
              " ' This',\n",
              " ' is',\n",
              " ' now',\n",
              " ' changing',\n",
              " ',',\n",
              " ' with',\n",
              " ' the',\n",
              " ' introduction',\n",
              " ' of',\n",
              " ' generative',\n",
              " ' A',\n",
              " '.I',\n",
              " '.s',\n",
              " ' like',\n",
              " ' OpenAI',\n",
              " '’s',\n",
              " ' GPT',\n",
              " '-',\n",
              " '4',\n",
              " '.\\n']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Uncomment and run this code:\n",
        "\n",
        "re.findall(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\", text_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMgHwT1xaNJf"
      },
      "source": [
        "After splitting the whole dataset into small chunks, we apply the algorithm on each chunk, that is, when we count the bigrams, we do not count bigrams from two neigboring chunks. See the pseudocode below:\n",
        "![BPETokenizer Preprocess](./images/BPE_preprocess.png)\n",
        "\n",
        "However, note that here the example shows everything on \"text\" space (\"hug\", \"pug\", \"i\" etc) for the sake of interpretability. In reality, we do these merging on the token space (v1, v2, v3 ...), ie, on the token ids.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "F5XUP1wfaNJg"
      },
      "outputs": [],
      "source": [
        "class BPETokenizerwithPreprocessing:\n",
        "    def __init__(self):\n",
        "        self.target_vocab_size = None\n",
        "        self.map_of_merged_tokens = None\n",
        "        self.tokenizer_map = None\n",
        "        self.map_of_char = None\n",
        "        self.split_pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "        # this is the split boundary pattern used in GPT4 tokenizer. Feel free to ask ChatGPT to explain what this regex pattern does.\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def replace_pairs(lst:list, bigram:tuple, c: int):\n",
        "\n",
        "        \"\"\"\n",
        "        Helper function to replace bigram with a single token in the list.\n",
        "        Whether to use this particular function or not depends on *your* implementation of the tokenizer.\n",
        "        It is possible that you may not need this function at all.\n",
        "        \"\"\"\n",
        "        i = 0\n",
        "        while i < len(lst) - 1:\n",
        "            if lst[i] == bigram[0] and lst[i + 1] == bigram[1]:\n",
        "                lst[i] = c\n",
        "                del lst[i + 1]\n",
        "            else:\n",
        "                i += 1\n",
        "        return lst\n",
        "\n",
        "\n",
        "    ### OPTIONAL (not graded) ####\n",
        "\n",
        "    # If you need, you can define another helper function to replace bigrams in the list of lists.\n",
        "\n",
        "\n",
        "    #### END ####\n",
        "    def preprocess_text(self, text: str):\n",
        "        return re.findall(self.split_pattern, text)\n",
        "\n",
        "    def train(self, text: str, target_vocab_size: int):\n",
        "\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "        character_set = set(text)\n",
        "        vocabsize = len(character_set)\n",
        "        map_of_char = {char: i for i, char in enumerate(character_set)}\n",
        "        self.map_of_char = map_of_char\n",
        "        map_of_merged_tokens = {}\n",
        "        print(f\"Preprocessing the text\")\n",
        "        preprocessed = self.preprocess_text(text)\n",
        "\n",
        "        #### TO DO: Write the loop and finish the implementation ####\n",
        "\n",
        "        self.tokenizer_map = {i: char for char, i in map_of_char.items()}\n",
        "        current_token_id = max(map_of_char.values()) + 1\n",
        "        tokenized_chunks = [[map_of_char[char] for char in chunk] for chunk in preprocessed]\n",
        "        while vocabsize < target_vocab_size:\n",
        "            bigram_counts = Counter()\n",
        "            for chunk in tokenized_chunks:\n",
        "                bigram_counts.update((chunk[i], chunk[i + 1]) for i in range(len(chunk) - 1))\n",
        "\n",
        "            if not bigram_counts:\n",
        "                break  # No more bigrams to process\n",
        "            most_frequent_bigram, _ = bigram_counts.most_common(1)[0]\n",
        "            for i, chunk in enumerate(tokenized_chunks):\n",
        "                tokenized_chunks[i] = self.replace_pairs(chunk, most_frequent_bigram, current_token_id)\n",
        "            map_of_merged_tokens[most_frequent_bigram] = current_token_id\n",
        "            self.tokenizer_map[current_token_id] = most_frequent_bigram\n",
        "            current_token_id += 1\n",
        "            vocabsize += 1\n",
        "\n",
        "        self.map_of_merged_tokens = map_of_merged_tokens\n",
        "        self.tokenizer_map = self.tokenizer_map # set this to the tokenizer_map you created in the loop above\n",
        "\n",
        "        #### END ####\n",
        "        self.map_of_merged_tokens = map_of_merged_tokens\n",
        "        self.tokenizer_map =self.tokenizer_map # set this to the tokenizer_map you created in the loop above\n",
        "\n",
        "\n",
        "    def encode_text(self, text: str):\n",
        "        \"\"\"Given a text, return the tokenized text\"\"\"\n",
        "        ### TO DO: Write the implementation of the encode_text function ###\n",
        "        tokenized = []\n",
        "        pos = 0\n",
        "        while pos < len(text):\n",
        "            match = None\n",
        "            for end in range(len(text), pos, -1):\n",
        "                subword = text[pos:end]\n",
        "                token_id = next((tid for tid, tsub in self.tokenizer_map.items() if tsub == subword), None)\n",
        "                if token_id is not None:\n",
        "                    match = token_id\n",
        "                    pos = end\n",
        "                    break\n",
        "            if match is None:\n",
        "                raise ValueError(f\"Subword '{text[pos]}' not found in tokenizer vocabulary.\")\n",
        "            tokenized.append(match)\n",
        "          #### END ####\n",
        "        return tokenized\n",
        "\n",
        "    def decode_text(self, tokenized_text: list):\n",
        "        \"\"\"Given a tokenized text, return the original text\"\"\"\n",
        "        ### TO DO: Write the implementation of the encode_text function ###\n",
        "        decoded_subwords = []\n",
        "        for token_id in tokenized_text:\n",
        "            decoded_subwords.append(self.tokenizer_map[token_id])\n",
        "        decode_text = \"\".join(decoded_subwords)\n",
        "        #### END ####\n",
        "        return decode_text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_ITSKYN1aNJg",
        "outputId": "79c45952-496e-406e-efd2-313abd270241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing the text\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Life is beautiful'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = BPETokenizerwithPreprocessing()\n",
        "tokenizer.train(text, 500)\n",
        "encoded = tokenizer.encode_text(\"Life is beautiful\")\n",
        "tokenizer.decode_text(encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "collapsed": true,
        "id": "UvnBruuhaNJg",
        "outputId": "43ee760d-e2f9-426c-da2e-130294677476"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Subword '인' not found in tokenizer vocabulary.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9013f44f8d90>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"인생은 아름답다\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# if you try Korean text, it will raise an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-9865a9aea579>\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Subword '{text[pos]}' not found in tokenizer vocabulary.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m           \u001b[0;31m#### END ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Subword '인' not found in tokenizer vocabulary."
          ]
        }
      ],
      "source": [
        "#encoded = tokenizer.encode_text(\"인생은 아름답다\") # if you try Korean text, it will raise an error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EjZbFJeaNJh"
      },
      "source": [
        "### Tokenizing in bytespace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_BkOqEmaNJh"
      },
      "source": [
        "So far we have only worked with English texts.\n",
        "\n",
        "But of course, there are thousands of other languages in the world, and not all languages can be represented in the way English is represented so far. This is more true for character based langauges like Chinese. Even for Korean language, building from the character level [like the way we did in English] is not possible. Which is why we need an universal way to represent languages of various scripts. Utf-8 provides such a way.\n",
        "\n",
        "Utf-8 is an encoding method that encodes **all** human written languages into variable length byte sequece of up to 4 bytes (256 bits). One way to think of it is to have an universal \"alphabet\" for all human written language, where the alphabet size is 256.\n",
        "\n",
        "However, the alphabet analogy is not quite right. To see why, see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKWx1lA6aNJh",
        "outputId": "8a8cff02-e81b-4220-da6a-560275f3a61f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Life is encoded as: [76, 105, 102, 101]\n",
            "L is encoded as: [76]\n",
            "i is encoded as: [105]\n",
            "f is encoded as: [102]\n",
            "e is encoded as: [101]\n",
            "생일 is encoded as: [236, 131, 157, 236, 157, 188]\n",
            "생 is encoded as: [236, 131, 157]\n",
            "ㅅ is encoded as: [227, 133, 133]\n",
            "새 is encoded as: [236, 131, 136]\n",
            "이 is encoded as: [236, 157, 180]\n",
            "일 is encoded as: [236, 157, 188]\n",
            "🎂 is encoded as: [240, 159, 142, 130]\n",
            "🤞 is encoded as: [240, 159, 164, 158]\n"
          ]
        }
      ],
      "source": [
        "# Let's play with uft-8 encoding\n",
        "s = \"Life\"\n",
        "print(f\"{s} is encoded as: {list(s.encode('utf-8'))}\")\n",
        "print(f\"L is encoded as: {list('L'.encode('utf-8'))}\")\n",
        "print(f\"i is encoded as: {list('i'.encode('utf-8'))}\")\n",
        "print(f\"f is encoded as: {list('f'.encode('utf-8'))}\")\n",
        "print(f\"e is encoded as: {list('e'.encode('utf-8'))}\")\n",
        "\n",
        "\n",
        "s = \"생일\"\n",
        "print(f\"{s} is encoded as: {list(s.encode('utf-8'))}\")\n",
        "print(f\"생 is encoded as: {list('생'.encode('utf-8'))}\")\n",
        "print(f\"ㅅ is encoded as: {list('ㅅ'.encode('utf-8'))}\")\n",
        "print(f\"새 is encoded as: {list('새'.encode('utf-8'))}\")\n",
        "print(f\"이 is encoded as: {list('이'.encode('utf-8'))}\")\n",
        "print(f\"일 is encoded as: {list('일'.encode('utf-8'))}\")\n",
        "\n",
        "s = \"🎂\"\n",
        "print(f\"{s} is encoded as: {list(s.encode('utf-8'))}\")\n",
        "\n",
        "s= \"🤞\"\n",
        "print(f\"{s} is encoded as: {list(s.encode('utf-8'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdoPbOj0aNJi"
      },
      "source": [
        "Did you see that utf-8 can also encode emojies and many languages? However, also note that the mapping from each alphabet to utf-8 encoding is not always very intuitive (look at the Korean example carefully). The reason English alphabet takes up lower values than non-English alphabet is because utf-8 is backward compatible (ASCII). For all non English languages, the encoded lists are usually long.\n",
        "\n",
        "Let's look at some more examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw_NVo05aNJi",
        "outputId": "77248e72-9686-4098-c4a5-099a77e66edd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded: 생일\n",
            "Decoded: ���일\n"
          ]
        }
      ],
      "source": [
        "# decoding\n",
        "encoded = [236, 131, 157, 236, 157, 188]\n",
        "print(f\"Decoded: {bytes(encoded).decode('utf-8', errors='replace')}\")\n",
        "\n",
        "encoded = [150, 131, 157, 236, 157, 188]\n",
        "print(f\"Decoded: {bytes(encoded).decode('utf-8', errors='replace')}\")\n",
        "\n",
        "# the second one is not a valid utf-8 encoding, so it will be replaced with a question mark\n",
        "# if your language model is not strong enough in modeling lanuguages, it will through more errors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGd5kKdgaNJj"
      },
      "source": [
        "To know more about utf-8 encoding, check out the corresponding Wikipedia page.\n",
        "\n",
        "Because of the obvious advantage of using utf-8 encoding, when we train a real encoder, we first project all the data into byte-space and then do the tokenizer training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkH8tfE7aNJj"
      },
      "source": [
        "### 4. ByteSpacePreprocessed\n",
        "BPE algorithm on byte space, however, the text first go through the regex preprocessing. Make sure to do the preprocessing first.\n",
        "\n",
        "Make sure to do the preprocessing **before** the byte encoding (and not after)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6qaBsoNOaNJj"
      },
      "outputs": [],
      "source": [
        "class BPETokenizerByteSpacePreprocessed:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.map_of_merged_tokens = None\n",
        "        self.tokenizer_map = None\n",
        "        self.map_of_char = None\n",
        "        self.split_pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "\n",
        "\n",
        "    ### OPTIONAL (not graded) ####\n",
        "\n",
        "    # If you need, you can define another helper function to replace bigrams in the list of lists.\n",
        "\n",
        "\n",
        "    #### END ####\n",
        "\n",
        "    @staticmethod\n",
        "    def replace_pairs(lst:list, bigram:tuple, c: int):\n",
        "\n",
        "        \"\"\"\n",
        "        Helper function to replace bigram with a single token in the list.\n",
        "        Whether to use this particular function or not depends on *your* implementation of the tokenizer.\n",
        "        It is possible that you may not need this function at all.\n",
        "        \"\"\"\n",
        "        i = 0\n",
        "        while i < len(lst) - 1:\n",
        "            if lst[i] == bigram[0] and lst[i + 1] == bigram[1]:\n",
        "                lst[i] = c\n",
        "                del lst[i + 1]\n",
        "            else:\n",
        "                i += 1\n",
        "        return lst\n",
        "\n",
        "    def preprocess_text(self, text: str):\n",
        "        return re.findall(self.split_pattern, text)\n",
        "\n",
        "    def train(self, text: str, target_vocab_size: int):\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "        character_set = set(range(256))  # All possible byte values\n",
        "        vocabsize = len(character_set)\n",
        "        assert self.target_vocab_size > vocabsize, \"target vocab size must be greater than 256\"\n",
        "\n",
        "        map_of_char = {char: i for i, char in enumerate(character_set)}\n",
        "        self.map_of_char = map_of_char\n",
        "\n",
        "        preprocessed = self.preprocess_text(text)\n",
        "        byte_chunks = [list(chunk.encode(\"utf-8\")) for chunk in preprocessed]\n",
        "\n",
        "        self.tokenizer_map = {i: (char,) for char, i in map_of_char.items()}\n",
        "\n",
        "        map_of_merged_tokens = {}\n",
        "        current_token_id = max(map_of_char.values()) + 1\n",
        "        while vocabsize < target_vocab_size:\n",
        "            bigram_counts = Counter()\n",
        "            for chunk in byte_chunks:\n",
        "                bigram_counts.update((chunk[i], chunk[i + 1]) for i in range(len(chunk) - 1))\n",
        "\n",
        "            if not bigram_counts:\n",
        "                break  # No more bigrams to merge\n",
        "\n",
        "            most_frequent_bigram, _ = bigram_counts.most_common(1)[0]\n",
        "\n",
        "            # Updating all chunks with the new merged token because necessary\n",
        "            for i, chunk in enumerate(byte_chunks):\n",
        "                byte_chunks[i] = self.replace_pairs(chunk, most_frequent_bigram, current_token_id)\n",
        "\n",
        "            # Update maps w new merged token\n",
        "            map_of_merged_tokens[most_frequent_bigram] = current_token_id\n",
        "            self.tokenizer_map[current_token_id] = most_frequent_bigram  # Store as tuple of ints\n",
        "\n",
        "            current_token_id += 1\n",
        "            vocabsize += 1\n",
        "        ### END ###\n",
        "        self.map_of_merged_tokens = map_of_merged_tokens\n",
        "        self.tokenizer_map = self.tokenizer_map\n",
        "\n",
        "\n",
        "    def encode_text(self, text: str):\n",
        "        \"\"\"Given a text, return the tokenized text\"\"\"\n",
        "        ### Implement the encode_text function ###\n",
        "        # You only need to sligthly modify existing code in your previous implementation\n",
        "        \"\"\"Given a text, return the tokenized text\"\"\"\n",
        "        preprocessed = self.preprocess_text(text)\n",
        "        byte_tokens = [list(chunk.encode(\"utf-8\")) for chunk in preprocessed]\n",
        "\n",
        "        tokenized = []\n",
        "        for chunk in byte_tokens:\n",
        "            i = 0\n",
        "            while i < len(chunk):\n",
        "                if i < len(chunk) - 1:\n",
        "                    bigram = (chunk[i], chunk[i + 1])\n",
        "                    if bigram in self.map_of_merged_tokens:\n",
        "                        tokenized.append(self.map_of_merged_tokens[bigram])\n",
        "                        i += 2\n",
        "                        continue\n",
        "                tokenized.append(chunk[i])\n",
        "                i += 1\n",
        "        ### END ###\n",
        "        return tokenized\n",
        "\n",
        "    def decode_text(self, tokenized_text: list):\n",
        "        \"\"\"Given a tokenized text, return the original text\"\"\"\n",
        "\n",
        "        ### TO DO: Write the implementation of the decode_text function ###\n",
        "        # make sure to use errors='replace' when decoding\n",
        "\n",
        "        decoded_bytes = []\n",
        "        for token_id in tokenized_text:\n",
        "            token_value = self.tokenizer_map.get(token_id)\n",
        "            if token_value is not None:\n",
        "                decoded_bytes.extend(token_value)  # token_value is a tuple of byte values\n",
        "            else:\n",
        "                pass\n",
        "        decoded_text = bytes(decoded_bytes).decode(\"utf-8\", errors=\"replace\")\n",
        "        return decoded_text\n",
        "\n",
        "\n",
        "        ### END ###\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXrIvTlhaNJk",
        "outputId": "ed8dcb75-1535-48be-dce1-40b6c22bd962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded: Life is beautiful 🤞\n",
            "Encoded: [76, 105, 102, 101, 32, 105, 115, 32, 98, 101, 97, 117, 116, 105, 102, 117, 108, 32, 240, 159, 164, 158]\n",
            "Decoded (with repr): 'Life is beautiful 🤞'\n",
            "Decoded (with repr): 'Life is beautiful 🤞'\n"
          ]
        }
      ],
      "source": [
        "# Sanity check\n",
        "\n",
        "with open(\"./kor_Emma.txt\", \"r\") as f: # grab some Korean text from Wikipedia\n",
        "    kor_text = f.read()\n",
        "\n",
        "tokenizer = BPETokenizerByteSpacePreprocessed()\n",
        "tokenizer.train(kor_text, 500)  # train the tokenizer on the Korean text\n",
        "encoded = tokenizer.encode_text(\"Life is beautiful 🤞\") # still manages to tokenize English\n",
        "decoded = tokenizer.decode_text(encoded)\n",
        "\n",
        "print(f\"Decoded: {decoded}\")\n",
        "encoded = tokenizer.encode_text(\"Life is beautiful 🤞\")\n",
        "print(f\"Encoded: {encoded}\")  # Inspect the tokenized output\n",
        "\n",
        "decoded = tokenizer.decode_text(encoded)\n",
        "print(f\"Decoded (with repr): {repr(decoded)}\")\n",
        "\n",
        "print(f\"Decoded (with repr): {repr(decoded)}\")  # Use repr to show any hidden whitespace characters\n",
        "\n",
        "assert decoded == \"Life is beautiful 🤞\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TXtWCiGaNJk",
        "outputId": "deea78e0-1ac9-4719-e01d-4c75eb636be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded Korean tokens: [236, 337, 257, 188, 256, 182, 149, 261, 152, 261, 180]\n",
            "Decoded Korean text (with repr): '생일 축하해'\n"
          ]
        }
      ],
      "source": [
        "# Try with Korean text;\n",
        "\n",
        "encoded = tokenizer.encode_text(\"생일 축하해\")\n",
        "print(f\"Encoded Korean tokens: {encoded}\")  # Check the tokenized output\n",
        "\n",
        "decoded = tokenizer.decode_text(encoded)\n",
        "print(f\"Decoded Korean text (with repr): {repr(decoded)}\")  # Display any hidden differences\n",
        "\n",
        "assert decoded == \"생일 축하해\"\n",
        "# and of course it works with Korean as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSikVDiDaNJk"
      },
      "outputs": [],
      "source": [
        "### TO DO ###\n",
        "\n",
        "# What are some distinctions you notice between the tokens produced by byte-level BPE with preprocessing vs vanilla BPE?\n",
        "\n",
        "# Train tokenizers using the Emma text first and do some analysis\n",
        "# Feel free to run some codes and show some examples, plot graphs etc.\n",
        "# citing papers/other resources is also fine, but make sure to explain the distinctions in your own words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjeMUdzPIDHm"
      },
      "source": [
        "After doing the comparison I find out that tokens are often readable subwords or characters, such as 'e', 'th', 'ing' under Vanilla BPE. A small number of tokens have very high frequencies, corresponding to common subwords. But for Byte-Level BPE tokens are byte sequences that may not directly correspond to readable characters, e.g., 'T', 'h', 'is'. There is a more uniform usage of tokens due to byte-level granularity. So in summary Vanilla BE is effective in producing meaningful tokens, which can be advantageous for language modeling tasks that benefit from understanding word structure. However, it may struggle with multilingual text not present in the training data.\n",
        "Byte-Level BPE with Preprocessing is good across different languages and character sets, as it operates at the byte level but then the sequences are longer. So what you want to use really depends on what your goal is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrIka75nKyX5"
      },
      "outputs": [],
      "source": [
        "eval_text_en = \"\"\"“Ever since the day—about four years ago—that Miss Taylor and I met\n",
        "with him in Broadway Lane, when, because it began to drizzle, he darted\n",
        "away with so much gallantry, and borrowed two umbrellas for us from\n",
        "Farmer Mitchell’s, I made up my mind on the subject. I planned the\n",
        "match from that hour; and when such success has blessed me in this\n",
        "instance, dear papa, you cannot think that I shall leave off\n",
        "match-making.”\n",
        "\n",
        "“I do not understand what you mean by ‘success,’” said Mr. Knightley.\n",
        "“Success supposes endeavour. Your time has been properly and delicately\n",
        "spent, if you have been endeavouring for the last four years to bring\n",
        "about this marriage. A worthy employment for a young lady’s mind! But\n",
        "if, which I rather imagine, your making the match, as you call it,\n",
        "means only your planning it, your saying to yourself one idle day, ‘I\n",
        "think it would be a very good thing for Miss Taylor if Mr. Weston were\n",
        "to marry her,’ and saying it again to yourself every now and then\n",
        "afterwards, why do you talk of success? Where is your merit? What are\n",
        "you proud of? You made a lucky guess; and _that_ is all that can be\n",
        "said.”\n",
        "\n",
        "“And have you never known the pleasure and triumph of a lucky guess?—I\n",
        "pity you.—I thought you cleverer—for, depend upon it a lucky guess is\n",
        "never merely luck. There is always some talent in it. And as to my poor\n",
        "word ‘success,’ which you quarrel with, I do not know that I am so\n",
        "entirely without any claim to it. You have drawn two pretty pictures;\n",
        "but I think there may be a third—a something between the do-nothing and\n",
        "the do-all. If I had not promoted Mr. Weston’s visits here, and given\n",
        "many little encouragements, and smoothed many little matters, it might\n",
        "not have come to any thing after all. I think you must know Hartfield\n",
        "enough to comprehend that.”\"\"\"\n",
        "\n",
        "eval_text_kor = \"\"\"\"4년 전쯤 테일러 양과 내가 브로드웨이 레인에서 그를 만난 날부터, 이슬비가 내리기 시작하자 그는 매우 용감하게 달려가서\n",
        "Farmer Mitchell's에서 우산 두 개를 빌렸을 때부터, 나는 그 문제에 대해 마음먹었습니다. 나는 그 순간부터\n",
        "결혼을 계획했습니다. 그리고 이런 성공이 이 경우에 나에게 축복이 되었을 때, 사랑하는 아빠, 당신은 내가\n",
        "결혼을 그만둘 것이라고 생각할 수 없습니다.\"\n",
        "\n",
        "\"나는 당신이 '성공'이라는 말로 무슨 뜻인지 이해하지 못합니다.\" 나이틀리 씨가 말했습니다.\n",
        "\"성공은 노력을 전제로 합니다. 지난 4년 동안 이 결혼을 이루기 위해 노력했다면, 당신의 시간은 적절하고 신중하게\n",
        "보내졌습니다. 젊은 여성의 마음에 가치 있는 일이었습니다! 하지만\n",
        "만약 당신이 결혼을 한다고 상상한다면, 당신이 말하는 대로\n",
        "그저 계획하는 것일 뿐이고, 어느 날 한가한 시간에 ‘웨스턴 씨가 테일러 양과 결혼하면 아주 좋은 일이 될 것 같아’라고 스스로에게 말하고, 그 후로 가끔씩 스스로에게 다시 말하는 것일 뿐이라면, 왜 성공에 대해 이야기하는 거지? 당신의 공로는 어디에 있지? 당신은 무엇을 자랑스러워하는 거지? 당신은 행운의 추측을 했고; _그게_ 말할 수 있는 전부야.\n",
        "\n",
        "\"그리고 당신은 행운의 추측의 즐거움과 승리를 결코 알지 못했니?—나는 당신을\n",
        "불쌍히 여긴다.—나는 당신이 더 똑똑하다고 생각했다—왜냐하면, 행운의 추측은\n",
        "결코 단순한 행운이 아니기 때문이다. 항상 어떤 재능이 거기에 있다. 그리고 당신이 다투는 나의 형편없는\n",
        "단어 ‘성공’에 대해 말하자면, 나는 내가 그것에 대한 권리가 전혀 없다는 것을 모른다. 당신은 두 개의 예쁜 그림을 그렸다.\n",
        "하지만 나는 세 번째 그림이 있을 수 있다고 생각한다—아무것도 하지 않는 것과\n",
        "모든 것을 하는 것 사이의 무언가. 내가 웨스턴 씨의 방문을 홍보하지 않았고,\n",
        "많은 작은 격려를 하지 않았고, 많은 작은 문제들을 해결하지 않았다면,\n",
        "결국 아무것도 이루어지지 않았을지도 모릅니다. 당신은 하트필드를\n",
        "충분히 알고 있을 테니 그걸 이해할 수 있을 겁니다.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "JJIoaNjFJo1u"
      },
      "outputs": [],
      "source": [
        "tokenizer1 = BPETokenizer()\n",
        "tokenizer2 = BPETokenizerByteSpacePreprocessed()\n",
        "tokenizer1.train(eval_text_en, 500)\n",
        "tokenizer2.train(eval_text_en, 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "DY7uJCyzI7xC",
        "outputId": "3720498b-d17b-4bce-ffd6-ba46868fbf2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing word: 'KAIST'\n",
            "Vanilla BPE Tokens: [48, 9, 23, 20, 40]\n",
            "Vanilla Decoded: 'KAIST'\n",
            "Byte-Level BPE Tokens: [75, 65, 73, 83, 84]\n",
            "Byte-Level Decoded: 'KAIST'\n",
            "\n",
            "Testing word: 'tree'\n",
            "Vanilla BPE Tokens: [27, 12, 35, 35]\n",
            "Vanilla Decoded: 'tree'\n",
            "Byte-Level BPE Tokens: [116, 367, 101]\n",
            "Byte-Level Decoded: 'tree'\n",
            "\n",
            "Testing word: 'déjà vu'\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'é'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-9f533c044ee5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTesting word: '{word}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Vanilla BPE Tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvanilla_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvanilla_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvanilla_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Vanilla BPE Tokens: {vanilla_encoded}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1e3e70a1d076>\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m### TO DO: Write the implementation of the encode_text function ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;34m\"\"\"Given a text, return the tokenized text as a list of integers\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_of_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1e3e70a1d076>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m### TO DO: Write the implementation of the encode_text function ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;34m\"\"\"Given a text, return the tokenized text as a list of integers\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_of_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'é'"
          ]
        }
      ],
      "source": [
        "test_words = [\"KAIST\", \"tree\", \"déjà vu\", \"こんにちは\"]\n",
        "\n",
        "for word in test_words:\n",
        "    print(f\"\\nTesting word: '{word}'\")\n",
        "    # Vanilla BPE Tokenization\n",
        "    vanilla_encoded = tokenizer1.encode_text(word)\n",
        "    vanilla_decoded = tokenizer1.decode_text(vanilla_encoded)\n",
        "    print(f\"Vanilla BPE Tokens: {vanilla_encoded}\")\n",
        "    print(f\"Vanilla Decoded: '{vanilla_decoded}'\")\n",
        "\n",
        "    # Byte-Level BPE Tokenization\n",
        "    byte_encoded = tokenizer2.encode_text(word)\n",
        "    byte_decoded = tokenizer2.decode_text(byte_encoded)\n",
        "    print(f\"Byte-Level BPE Tokens: {byte_encoded}\")\n",
        "    print(f\"Byte-Level Decoded: '{byte_decoded}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTs2SNmfaNJk"
      },
      "source": [
        "# Multilingual Tokenization analysis:\n",
        "\n",
        "You are given two files in the ./multilingual-data folder. The English one is a chunk of the original English Emma novel. The Korean one is machine translated version of that. Now you will think about some properties of the tokenizers when trained on different corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Wdoj5R7YaNJl"
      },
      "outputs": [],
      "source": [
        "with open(\"./kor_Emma.txt\", \"r\") as f:\n",
        "    kor_emma = f.read()\n",
        "\n",
        "with open(\"./en_Emma.txt\", \"r\") as f:\n",
        "    eng_emma = f.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cbnPB2fyaNJu"
      },
      "outputs": [],
      "source": [
        "eval_text_en = \"\"\"“Ever since the day—about four years ago—that Miss Taylor and I met\n",
        "with him in Broadway Lane, when, because it began to drizzle, he darted\n",
        "away with so much gallantry, and borrowed two umbrellas for us from\n",
        "Farmer Mitchell’s, I made up my mind on the subject. I planned the\n",
        "match from that hour; and when such success has blessed me in this\n",
        "instance, dear papa, you cannot think that I shall leave off\n",
        "match-making.”\n",
        "\n",
        "“I do not understand what you mean by ‘success,’” said Mr. Knightley.\n",
        "“Success supposes endeavour. Your time has been properly and delicately\n",
        "spent, if you have been endeavouring for the last four years to bring\n",
        "about this marriage. A worthy employment for a young lady’s mind! But\n",
        "if, which I rather imagine, your making the match, as you call it,\n",
        "means only your planning it, your saying to yourself one idle day, ‘I\n",
        "think it would be a very good thing for Miss Taylor if Mr. Weston were\n",
        "to marry her,’ and saying it again to yourself every now and then\n",
        "afterwards, why do you talk of success? Where is your merit? What are\n",
        "you proud of? You made a lucky guess; and _that_ is all that can be\n",
        "said.”\n",
        "\n",
        "“And have you never known the pleasure and triumph of a lucky guess?—I\n",
        "pity you.—I thought you cleverer—for, depend upon it a lucky guess is\n",
        "never merely luck. There is always some talent in it. And as to my poor\n",
        "word ‘success,’ which you quarrel with, I do not know that I am so\n",
        "entirely without any claim to it. You have drawn two pretty pictures;\n",
        "but I think there may be a third—a something between the do-nothing and\n",
        "the do-all. If I had not promoted Mr. Weston’s visits here, and given\n",
        "many little encouragements, and smoothed many little matters, it might\n",
        "not have come to any thing after all. I think you must know Hartfield\n",
        "enough to comprehend that.”\"\"\"\n",
        "\n",
        "eval_text_kor = \"\"\"\"4년 전쯤 테일러 양과 내가 브로드웨이 레인에서 그를 만난 날부터, 이슬비가 내리기 시작하자 그는 매우 용감하게 달려가서\n",
        "Farmer Mitchell's에서 우산 두 개를 빌렸을 때부터, 나는 그 문제에 대해 마음먹었습니다. 나는 그 순간부터\n",
        "결혼을 계획했습니다. 그리고 이런 성공이 이 경우에 나에게 축복이 되었을 때, 사랑하는 아빠, 당신은 내가\n",
        "결혼을 그만둘 것이라고 생각할 수 없습니다.\"\n",
        "\n",
        "\"나는 당신이 '성공'이라는 말로 무슨 뜻인지 이해하지 못합니다.\" 나이틀리 씨가 말했습니다.\n",
        "\"성공은 노력을 전제로 합니다. 지난 4년 동안 이 결혼을 이루기 위해 노력했다면, 당신의 시간은 적절하고 신중하게\n",
        "보내졌습니다. 젊은 여성의 마음에 가치 있는 일이었습니다! 하지만\n",
        "만약 당신이 결혼을 한다고 상상한다면, 당신이 말하는 대로\n",
        "그저 계획하는 것일 뿐이고, 어느 날 한가한 시간에 ‘웨스턴 씨가 테일러 양과 결혼하면 아주 좋은 일이 될 것 같아’라고 스스로에게 말하고, 그 후로 가끔씩 스스로에게 다시 말하는 것일 뿐이라면, 왜 성공에 대해 이야기하는 거지? 당신의 공로는 어디에 있지? 당신은 무엇을 자랑스러워하는 거지? 당신은 행운의 추측을 했고; _그게_ 말할 수 있는 전부야.\n",
        "\n",
        "\"그리고 당신은 행운의 추측의 즐거움과 승리를 결코 알지 못했니?—나는 당신을\n",
        "불쌍히 여긴다.—나는 당신이 더 똑똑하다고 생각했다—왜냐하면, 행운의 추측은\n",
        "결코 단순한 행운이 아니기 때문이다. 항상 어떤 재능이 거기에 있다. 그리고 당신이 다투는 나의 형편없는\n",
        "단어 ‘성공’에 대해 말하자면, 나는 내가 그것에 대한 권리가 전혀 없다는 것을 모른다. 당신은 두 개의 예쁜 그림을 그렸다.\n",
        "하지만 나는 세 번째 그림이 있을 수 있다고 생각한다—아무것도 하지 않는 것과\n",
        "모든 것을 하는 것 사이의 무언가. 내가 웨스턴 씨의 방문을 홍보하지 않았고,\n",
        "많은 작은 격려를 하지 않았고, 많은 작은 문제들을 해결하지 않았다면,\n",
        "결국 아무것도 이루어지지 않았을지도 모릅니다. 당신은 하트필드를\n",
        "충분히 알고 있을 테니 그걸 이해할 수 있을 겁니다.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziE0WzRvPrAF"
      },
      "source": [
        "TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "M3MUA4dvDn-Q"
      },
      "outputs": [],
      "source": [
        "tokenizer_kr = BPETokenizerByteSpacePreprocessed()\n",
        "tokenizer_kr.train(kor_emma, target_vocab_size=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "mn-dnccoEKbP"
      },
      "outputs": [],
      "source": [
        "tokenizer_en = BPETokenizerByteSpacePreprocessed()\n",
        "tokenizer_en.train(eng_emma, target_vocab_size=1000)  # This is where i adjust vocab size for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lgj9hERdDr5_"
      },
      "outputs": [],
      "source": [
        "combined_text = eng_emma + \"\\n\" + kor_emma\n",
        "tokenizer_combined = BPETokenizerByteSpacePreprocessed()\n",
        "tokenizer_combined.train(combined_text, target_vocab_size=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF9GGcM0EB_b",
        "outputId": "cce4a835-ef1a-4083-b31e-12c1dcaca2f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English tokenizer works correctly on English text.\n"
          ]
        }
      ],
      "source": [
        "sample_text_en = eval_text_en\n",
        "encoded_en = tokenizer_en.encode_text(sample_text_en)\n",
        "decoded_en = tokenizer_en.decode_text(encoded_en)\n",
        "assert decoded_en == sample_text_en\n",
        "print(\"English tokenizer works correctly on English text.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZZXsOErEN5o",
        "outputId": "913f60a0-2afd-46b9-b704-e184fe19ee27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Korean tokenizer works correctly on Korean text.\n"
          ]
        }
      ],
      "source": [
        "sample_text_kr = eval_text_kor\n",
        "encoded_kr = tokenizer_kr.encode_text(sample_text_kr)\n",
        "decoded_kr = tokenizer_kr.decode_text(encoded_kr)\n",
        "assert decoded_kr == sample_text_kr\n",
        "print(\"Korean tokenizer works correctly on Korean text.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8y3I9ASER-Y",
        "outputId": "48c82831-7f0c-43c6-ff20-f758b0bcb6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined tokenizer works correctly on English text.\n",
            "Combined tokenizer works correctly on Korean text.\n"
          ]
        }
      ],
      "source": [
        "encoded_combined_en = tokenizer_combined.encode_text(sample_text_en)\n",
        "decoded_combined_en = tokenizer_combined.decode_text(encoded_combined_en)\n",
        "assert decoded_combined_en == sample_text_en\n",
        "print(\"Combined tokenizer works correctly on English text.\")\n",
        "encoded_combined_kr = tokenizer_combined.encode_text(sample_text_kr)\n",
        "decoded_combined_kr = tokenizer_combined.decode_text(encoded_combined_kr)\n",
        "assert decoded_combined_kr == sample_text_kr\n",
        "print(\"Combined tokenizer works correctly on Korean text.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JRvdPQXkEU0c",
        "outputId": "1bdec4ab-1a66-45bd-8c4f-5c484c814ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample tokens from English tokenizer:\n",
            "Token ID 0: '\\x00'\n",
            "Token ID 1: '\\x01'\n",
            "Token ID 2: '\\x02'\n",
            "Token ID 3: '\\x03'\n",
            "Token ID 4: '\\x04'\n",
            "Token ID 5: '\\x05'\n",
            "Token ID 6: '\\x06'\n",
            "Token ID 7: '\\x07'\n",
            "Token ID 8: '\\x08'\n",
            "Token ID 9: '\\t'\n",
            "Token ID 10: '\\n'\n",
            "Token ID 11: '\\x0b'\n",
            "Token ID 12: '\\x0c'\n",
            "Token ID 13: '\\r'\n",
            "Token ID 14: '\\x0e'\n",
            "Token ID 15: '\\x0f'\n",
            "Token ID 16: '\\x10'\n",
            "Token ID 17: '\\x11'\n",
            "Token ID 18: '\\x12'\n",
            "Token ID 19: '\\x13'\n"
          ]
        }
      ],
      "source": [
        "vocab_en = tokenizer_en.tokenizer_map\n",
        "print(\"\\nSample tokens from English tokenizer:\")\n",
        "for token_id in list(vocab_en.keys())[:20]:\n",
        "    token = vocab_en[token_id]\n",
        "    token_bytes = bytes(token)\n",
        "    token_str = token_bytes.decode('utf-8', errors='replace')\n",
        "    print(f\"Token ID {token_id}: {repr(token_str)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8kvcP5OCEXHa",
        "outputId": "0ad4d1e3-7774-42f3-def9-942ec0b32df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample tokens from Korean tokenizer:\n",
            "Token ID 0: '\\x00'\n",
            "Token ID 1: '\\x01'\n",
            "Token ID 2: '\\x02'\n",
            "Token ID 3: '\\x03'\n",
            "Token ID 4: '\\x04'\n",
            "Token ID 5: '\\x05'\n",
            "Token ID 6: '\\x06'\n",
            "Token ID 7: '\\x07'\n",
            "Token ID 8: '\\x08'\n",
            "Token ID 9: '\\t'\n",
            "Token ID 10: '\\n'\n",
            "Token ID 11: '\\x0b'\n",
            "Token ID 12: '\\x0c'\n",
            "Token ID 13: '\\r'\n",
            "Token ID 14: '\\x0e'\n",
            "Token ID 15: '\\x0f'\n",
            "Token ID 16: '\\x10'\n",
            "Token ID 17: '\\x11'\n",
            "Token ID 18: '\\x12'\n",
            "Token ID 19: '\\x13'\n"
          ]
        }
      ],
      "source": [
        "vocab_kr = tokenizer_kr.tokenizer_map\n",
        "\n",
        "print(\"\\nSample tokens from Korean tokenizer:\")\n",
        "for token_id in list(vocab_kr.keys())[:20]:\n",
        "    token = vocab_kr[token_id]\n",
        "    token_bytes = bytes(token)\n",
        "    token_str = token_bytes.decode('utf-8', errors='replace')\n",
        "    print(f\"Token ID {token_id}: {repr(token_str)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "piKCyfuAEYt9",
        "outputId": "48c614ea-a881-40db-a476-2e4f8a329187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample tokens from combined tokenizer:\n",
            "Token ID 0: '\\x00'\n",
            "Token ID 1: '\\x01'\n",
            "Token ID 2: '\\x02'\n",
            "Token ID 3: '\\x03'\n",
            "Token ID 4: '\\x04'\n",
            "Token ID 5: '\\x05'\n",
            "Token ID 6: '\\x06'\n",
            "Token ID 7: '\\x07'\n",
            "Token ID 8: '\\x08'\n",
            "Token ID 9: '\\t'\n",
            "Token ID 10: '\\n'\n",
            "Token ID 11: '\\x0b'\n",
            "Token ID 12: '\\x0c'\n",
            "Token ID 13: '\\r'\n",
            "Token ID 14: '\\x0e'\n",
            "Token ID 15: '\\x0f'\n",
            "Token ID 16: '\\x10'\n",
            "Token ID 17: '\\x11'\n",
            "Token ID 18: '\\x12'\n",
            "Token ID 19: '\\x13'\n"
          ]
        }
      ],
      "source": [
        "vocab_combined = tokenizer_combined.tokenizer_map\n",
        "\n",
        "print(\"\\nSample tokens from combined tokenizer:\")\n",
        "for token_id in list(vocab_combined.keys())[:20]:\n",
        "    token = vocab_combined[token_id]\n",
        "    token_bytes = bytes(token)\n",
        "    token_str = token_bytes.decode('utf-8', errors='replace')\n",
        "    print(f\"Token ID {token_id}: {repr(token_str)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIuSavqUPuJ2"
      },
      "source": [
        "TESTING END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiI2tjm4aNJv"
      },
      "outputs": [],
      "source": [
        "### TODO: Open Ended\n",
        "\n",
        "# Play around with the BPETokenizerByteSpacePreprocessed that you implemented using the Korean and English Emma data, which you can use for training the tokenizers, and write down if you see anything interesting.\n",
        "\n",
        "# What does your findings imply for designing multilingual tokenizers?\n",
        "\n",
        "# Feel free to nudge vocabulary size as well. You can use the eval_text in the above cell for validation.\n",
        "\n",
        "# Do not hesitate to look into the actual tokens and see how much of it makes sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiD2e6zDE7dR"
      },
      "source": [
        "Seeing these results, I see that tokenizer performance depends on data as the tokenizer performs best on the data it was trained on. Also for english, the tokenizer merges frequent byte pairs corresponding to common English letter combinations (e.g., 'th', 'he', 'in'). But Korean characters are encoded in multiple bytes and the tokenizer merges byte pairs within characters. For the combined tokenizer, The tokenizer includes tokens from both languages and performs good enough as combined assertion works well too. Thus this implies that when designing multilingual tokenizers, its important to have a balanced training data consisting of both languages like I used emma and both korean and english here.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KymPEo-GaNJw"
      },
      "source": [
        "## Wordpiece\n",
        "\n",
        "Now, implement the wordpiece tokenizer algorithm, which is almost similar to BPE, except the objective function is a bit different.\n",
        "\n",
        "Check the description below.\n",
        "\n",
        "![Wordpiece Algorithm](./images/Wordpiece.png)\n",
        "\n",
        "\n",
        "Make sure to:\n",
        "1. Preprocess the text first\n",
        "2. Work on the byte-space (utf-8 encoding)\n",
        "3. When you estimate p(v_i, v_j), divide by N-1, instead of N where N is the total number of tokens in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RMfE5nI2aNJw"
      },
      "outputs": [],
      "source": [
        "class WordPieceTokenizer:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.map_of_merged_tokens = None\n",
        "        self.tokenizer_map = None\n",
        "        self.map_of_char = None\n",
        "        self.split_pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "\n",
        "    ### OPTIONAL ####\n",
        "    # You can define another helper function to replace bigrams in the list of lists.\n",
        "\n",
        "\n",
        "    #### END ####\n",
        "\n",
        "    @staticmethod\n",
        "    def replace_pairs(lst:list, bigram:tuple, c: int):\n",
        "\n",
        "        \"\"\"\n",
        "        Helper function to replace bigram with a single token in the list.\n",
        "        Whether to use this particular function or not depends on *your* implementation of the tokenizer.\n",
        "        It is possible that you may not need this function at all.\n",
        "        \"\"\"\n",
        "        i = 0\n",
        "        while i < len(lst) - 1:\n",
        "            if lst[i] == bigram[0] and lst[i + 1] == bigram[1]:\n",
        "                lst[i] = c\n",
        "                del lst[i + 1]\n",
        "            else:\n",
        "                i += 1\n",
        "        return lst\n",
        "\n",
        "    def preprocess_text(self, text: str):\n",
        "        return re.findall(self.split_pattern, text)\n",
        "\n",
        "    def train(self, text: str, target_vocab_size: int):\n",
        "\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "\n",
        "\n",
        "        character_set = set([i for i in range(256)]) # We start with all the possible bytes, which is 256\n",
        "        vocabsize = len(character_set)\n",
        "        assert self.target_vocab_size > vocabsize, \"target vocab size must be greater than 256\"\n",
        "\n",
        "        map_of_char = {char: i for i, char in enumerate(character_set)}\n",
        "        self.map_of_char = map_of_char\n",
        "\n",
        "        map_of_merged_tokens = {}\n",
        "\n",
        "        ### TODO: Your implementation of wordpiece training goes here. ###\n",
        "        # Make sure to preprocess the text first\n",
        "        self.tokenizer_map = {char: (char,) for char in character_set}\n",
        "        preprocessed = self.preprocess_text(text)\n",
        "        byte_chunks = [list(chunk.encode(\"utf-8\")) for chunk in preprocessed]\n",
        "        corpus = [chunk[:] for chunk in byte_chunks]  # Copy of byte_chunks\n",
        "\n",
        "        current_token_id = 256\n",
        "\n",
        "        while vocabsize < self.target_vocab_size:\n",
        "            tokens = [token for chunk in corpus for token in chunk]\n",
        "            N = len(tokens)\n",
        "\n",
        "            if N < 2:\n",
        "                break  # Not enough tokens to form bigrams\n",
        "\n",
        "            # Counting all the tokens and bigrams\n",
        "            token_counts = Counter(tokens)\n",
        "            bigram_counts = Counter()\n",
        "            for chunk in corpus:\n",
        "                bigram_counts.update(zip(chunk, chunk[1:]))\n",
        "            delta_L_dict = {}\n",
        "            for bigram, c_vi_vj in bigram_counts.items():\n",
        "                vi, vj = bigram\n",
        "                c_vi = token_counts[vi]\n",
        "                c_vj = token_counts[vj]\n",
        "                p_vi_vj = c_vi_vj / (N - 1)\n",
        "                p_vi = c_vi / N\n",
        "                p_vj = c_vj / N\n",
        "                delta_L = c_vi_vj * math.log(p_vi_vj / (p_vi * p_vj) + 1e-8)\n",
        "                delta_L_dict[bigram] = delta_L\n",
        "\n",
        "            if not delta_L_dict:\n",
        "                break\n",
        "            best_bigram, max_delta_L = max(delta_L_dict.items(), key=lambda item: item[1])\n",
        "\n",
        "            for i, chunk in enumerate(corpus):\n",
        "                corpus[i] = self.replace_pairs(chunk, best_bigram, current_token_id)\n",
        "            # Updating the tokenizer_map with new token\n",
        "            vi, vj = best_bigram\n",
        "            bytes_vi = self.tokenizer_map[vi]\n",
        "            bytes_vj = self.tokenizer_map[vj]\n",
        "            new_token_bytes = bytes_vi + bytes_vj\n",
        "            self.tokenizer_map[current_token_id] = new_token_bytes\n",
        "\n",
        "            if self.map_of_merged_tokens is None:\n",
        "                self.map_of_merged_tokens = {}\n",
        "            self.map_of_merged_tokens[best_bigram] = current_token_id\n",
        "            current_token_id += 1\n",
        "            vocabsize += 1\n",
        "\n",
        "        ### END ###\n",
        "\n",
        "        self.map_of_merged_tokens = map_of_merged_tokens\n",
        "        self.tokenizer_map = self.tokenizer_map # set this to the tokenizer_map you created in the loop above\n",
        "\n",
        "\n",
        "    def encode_text(self, text: str):\n",
        "        \"\"\"Given a text, return the tokenized text\"\"\"\n",
        "        ### Implement the encode_text function ###\n",
        "        # You only need to sligthly modify existing code in your previous implementation\n",
        "        preprocessed = self.preprocess_text(text)\n",
        "        byte_chunks = [chunk.encode(\"utf-8\") for chunk in preprocessed]\n",
        "        byte_seq_to_token_id = {v: k for k, v in self.tokenizer_map.items()}\n",
        "\n",
        "        tokenized = []\n",
        "        for byte_chunk in byte_chunks:\n",
        "            i = 0\n",
        "            while i < len(byte_chunk):\n",
        "                matched = False\n",
        "                # Startiing from longest possible substring\n",
        "                for j in range(len(byte_chunk), i, -1):\n",
        "                    sub_bytes = byte_chunk[i:j]\n",
        "                    sub_bytes_tuple = tuple(sub_bytes)\n",
        "                    if sub_bytes_tuple in byte_seq_to_token_id:\n",
        "                        token_id = byte_seq_to_token_id[sub_bytes_tuple]\n",
        "                        tokenized.append(token_id)\n",
        "                        i = j\n",
        "                        matched = True\n",
        "                        break\n",
        "                if not matched:\n",
        "                    token_id = byte_chunk[i]\n",
        "                    tokenized.append(token_id)\n",
        "                    i += 1\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    def decode_text(self, tokenized_text: list):\n",
        "        \"\"\"Given a tokenized text, return the original text\"\"\"\n",
        "\n",
        "        ### TO DO: Write the implementation of the decode_text function ###\n",
        "        decoded_bytes = []\n",
        "        for token_id in tokenized_text:\n",
        "            token_value = self.tokenizer_map.get(token_id)\n",
        "            if token_value is not None:\n",
        "                decoded_bytes.extend(token_value)\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        decoded = bytes(decoded_bytes).decode(\"utf-8\", errors=\"replace\")\n",
        "        return decoded\n",
        "        # make sure to use errors='replace' when decoding\n",
        "\n",
        "        ### END ###\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "PZsz5q5kaNJw"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "tokenizer = WordPieceTokenizer()\n",
        "tokenizer.train(text, 500)\n",
        "encoded = tokenizer.encode_text(\"Life is beautiful 🤞\")\n",
        "decoded = tokenizer.decode_text(encoded)\n",
        "\n",
        "assert decoded == \"Life is beautiful 🤞\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aimlhw1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
